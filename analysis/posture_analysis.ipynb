{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.transform import Rotation\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"J:\\Alja Podgornik\\Multimaze arena\\Cohort 1_June 2020\\all_videos\\processed\"\n",
    "\n",
    "# Load the combined tracking data if exists\n",
    "if os.path.exists(os.path.join(folder, 'combined_tracking.h5')):\n",
    "    print(os.path.join(folder, 'combined_tracking.h5'), 'found.', 'Loading..')\n",
    "    df = pd.read_hdf(os.path.join(folder, 'combined_tracking.h5'))\n",
    "    print('Loaded to the memory')\n",
    "else: # Otherwise create and save the combined tracking data\n",
    "    print(os.path.join(folder, 'combined_tracking.h5'), 'not found.', 'Creating..')\n",
    "    data = {}\n",
    "    for file in tqdm(os.listdir(folder)):\n",
    "        if 'DLC' in file and file.endswith('.h5'):\n",
    "            # Prepare group, animal, day information\n",
    "            filename = file[:file.find('DLC')]\n",
    "            splitted_name = filename.rsplit(sep='_')\n",
    "            if len(splitted_name) == 2:\n",
    "                animal = splitted_name[0]\n",
    "                day = int(splitted_name[1][-1])\n",
    "                if animal.startswith('chr'):\n",
    "                    group = 'chr'\n",
    "                elif animal.startswith('hr'):\n",
    "                    group = 'hr'\n",
    "                elif animal.startswith('ctrl'):\n",
    "                    group = 'ctrl'\n",
    "\n",
    "            # Read in the DLC tracking data and append group, animal, day information\n",
    "            tracking = pd.read_hdf(os.path.join(folder, file))\n",
    "            tracking = tracking.droplevel(level=0, axis=1)\n",
    "            tracking['group'] = group\n",
    "            tracking['animal'] = animal\n",
    "            tracking['day'] = day\n",
    "            # Append into the dictionary to concatenate later\n",
    "            data[file] = tracking\n",
    "\n",
    "    df = pd.concat(data.values()).reset_index().rename({'index':'frame'}, axis='columns')\n",
    "    # Change dtypes for compactness\n",
    "    group_dtype = pd.CategoricalDtype(categories=['chr', 'ctrl', 'hr'], ordered=True)\n",
    "    dtypes = {'group':group_dtype, 'animal':'category', 'day':'category'}\n",
    "    df.astype(dtypes)\n",
    "    # Write to disk\n",
    "    df = df.set_index(['group', 'animal', 'day', 'frame']).sort_index()\n",
    "    df.to_hdf(os.path.join(folder, 'combined_tracking.h5'), key='combined_tracking')\n",
    "    print(os.path.join(folder, 'combined_tracking.h5'), 'created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = df.copy(deep=True)\n",
    "\n",
    "# Filtering out bad tracking data\n",
    "df = df.drop(columns=['paw_f_left', 'paw_f_right', 'paw_h_left', 'paw_h_right', 'tail_tip', 'tail_mid'], level=0)\n",
    "high_likelihood = df.loc[:, df.columns.get_level_values(1) == 'likelihood'] > 0.9\n",
    "df = df.loc[high_likelihood.all(axis=1), :]\n",
    "# df = df.drop(columns='likelihood', level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering the mouse to the origin\n",
    "center = df['center']\n",
    "unique_bodyparts = df.columns.get_level_values(0).unique()\n",
    "unique_axes = df.columns.get_level_values(1).unique()\n",
    "xy = unique_axes[:-1]\n",
    "\n",
    "for bodypart in tqdm(unique_bodyparts):\n",
    "    for axis in xy:\n",
    "        df.loc[:, (bodypart, axis)] = df.loc[:, (bodypart, axis)] - center.loc[:, axis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotating the mouse\n",
    "rotated = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "angles = np.arctan2(df[('tail_base', 'y')], df['tail_base', 'x'])\n",
    "\n",
    "for bodypart in tqdm(unique_bodyparts):\n",
    "    bp_df = df.loc[:, bodypart]\n",
    "    bp_x = bp_df['x']\n",
    "    bp_y = bp_df['y']\n",
    "    bp_likelihood = bp_df['likelihood']\n",
    "    cos = np.cos(angles)\n",
    "    sin = np.sin(angles)\n",
    "    for axis in bp_df.columns:\n",
    "        if axis == 'x':\n",
    "            rotated.loc[:, (bodypart, axis)] = (bp_x * cos) + (bp_y * sin)\n",
    "        elif axis == 'y':\n",
    "            rotated.loc[:, (bodypart, axis)] = (bp_y * cos) - (bp_x * sin)\n",
    "        elif axis == 'likelihood':\n",
    "            rotated.loc[:, (bodypart, axis)] = bp_likelihood\n",
    "\n",
    "rotated = rotated.drop(columns='likelihood', level=1)\n",
    "df = rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(whiten=True, svd_solver='full')\n",
    "\n",
    "pca_transformed = pca.fit_transform(df)\n",
    "\n",
    "expVar = pd.DataFrame(data=pca.explained_variance_ratio_, columns=['explained variance ratio'])\n",
    "expVar\n",
    "sns.barplot(data=expVar, y='explained variance ratio', x=list(range(1, len(expVar)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformed_df = pd.DataFrame(data=pca_transformed, columns=list(range(1, len(expVar)+1)),\n",
    "                               index=df.index)\n",
    "pca_transformed_df = pca_transformed_df.iloc[:, :11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = np.arange(1000, 12001, 1000)\n",
    "# n=index\n",
    "# timing = pd.DataFrame(data=n, columns=['n'], index=index)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# import time\n",
    "\n",
    "# tsne = TSNE(n_components=2, perplexity=50, n_iter=20000, n_iter_without_progress=500, init='random', n_jobs=-1, random_state=1)\n",
    "\n",
    "# for i in tqdm(timing.index):\n",
    "#     subset = pca_transformed_df.sample(timing.loc[i, 'n'])\n",
    "#     start_time = time.time()\n",
    "#     transformed = tsne.fit_transform(subset)\n",
    "#     timing.loc[i, 'fit_transform'] = (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(data=timing, x='n', y='fit_transform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# polynomial_features= PolynomialFeatures(degree=2)\n",
    "# x_poly = polynomial_features.fit_transform(timing['n'][:, np.newaxis])\n",
    "# y = timing['fit_transform'][:, np.newaxis]\n",
    "\n",
    "# model = LinearRegression()\n",
    "# model.fit(x_poly, y)\n",
    "# y_poly_pred = model.predict(x_poly)\n",
    "\n",
    "# rmse = np.sqrt(mean_squared_error(y,y_poly_pred))\n",
    "# r2 = r2_score(y,y_poly_pred)\n",
    "# print(rmse)\n",
    "# print(r2)\n",
    "\n",
    "# newpred = np.arange(1000000, 15000001, 500000)\n",
    "# newpred_poly = polynomial_features.fit_transform(newpred[:, np.newaxis])\n",
    "# new_y_pred = model.predict(newpred_poly)\n",
    "\n",
    "# plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(timing['n'], timing['fit_transform']/(60*60))\n",
    "# plt.plot(timing['n'], y_poly_pred/(60*60),'r--')\n",
    "# plt.legend(['r\\u00b2= ' + str(round(r2, 3))])\n",
    "# plt.xlabel('Number of datapoints')\n",
    "# plt.ylabel('Embedding time (h/16 cores)')\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(newpred, new_y_pred/(60*60), 'r--')\n",
    "# plt.xlabel('Number of datapoints')\n",
    "# plt.ylabel(\"'Estimated' embedding time (h/16 cores)\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(r\"C:\\Users\\serce\\Desktop\\tsne_timing.pdf\")\n",
    "# print(\"'Estimated' computation time on 15 million datapoints is\",\n",
    "#       (model.predict(polynomial_features.fit_transform(np.array([[15000000]]))))/(60*60),\n",
    "#      'hours on 16 cores')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newpred = np.arange(20000, 250000, 1000)\n",
    "# newpred_poly = polynomial_features.fit_transform(newpred[:, np.newaxis])\n",
    "# new_y_pred = model.predict(newpred_poly)\n",
    "\n",
    "# print(model.coef_)\n",
    "# plt.figure()\n",
    "# plt.plot(newpred, new_y_pred/(60*60))\n",
    "# plt.xlabel('Number of datapoints')\n",
    "# plt.ylabel('Estimated computation time (h/16 cores)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Estimated' computation time on 15 million datapoints is [[38041.84420021]] hours on 16 cores\n",
    "\n",
    "'Estimated' computation time on 200k datapoints is ~10 hours on 16 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import time\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=100, verbose=True, n_epochs=1000, min_dist=0)\n",
    "start_time = time.time()\n",
    "transformed = reducer.fit_transform(df)\n",
    "end_time = time.time() - start_time\n",
    "print('UMAP fit_transform took {} seconds.'.format(end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_df = pd.DataFrame(data=transformed, index=df.index, columns=['UMAP1', 'UMAP2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=umap_df, x='UMAP1', y='UMAP2', size=0.00001, legend=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=1.5)\n",
    "start_time = time.time()\n",
    "kde.fit(transformed)\n",
    "print(\"fitted in --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "scores = kde.score_samples(transformed)\n",
    "print(\"scored in --- %s seconds ---\" % (time.time() - start_time))\n",
    "transformed['score'] = np.exp(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:behaviour-switching] *",
   "language": "python",
   "name": "conda-env-behaviour-switching-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
